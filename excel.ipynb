{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem: Data is in excel and you need json objects as input\n",
    "\n",
    "Solution: The below code converts Excel rows to a JSON string using python pandas lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas\n",
    "\n",
    "excel_data_Enrol = pandas.read_excel('filename.xlsx', sheet_name='Enrolment')\n",
    "excel_data_Acad = pandas.read_excel('filename.xlsx', sheet_name='Academics')\n",
    "\n",
    "json_str_Enrol = excel_data_Enrol.to_json(orient='records')\n",
    "json_str_Acad = excel_data_Acad.to_json(orient='records')\n",
    "\n",
    "print('Enrolment Data to JSON:\\n', json_str_Enrol, '\\n\\n')\n",
    "print('Academic Data to JSON:\\n', json_str_Acad, '\\n\\n')\n",
    "\n",
    "data = {\"analytics\":[{\"enrolment\":json.loads(json_str_Enrol)},{\"academics\":json.loads(json_str_Acad)}]}\n",
    "json_data = json.dumps(data)\n",
    "# Write the data to a text file\n",
    "with open('output.txt', 'w') as f:\n",
    "        f.write(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scrap data on mps from http://www.parliament.go.ke/the-national-assembly/mps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bs4 import BeautifulSoup\n",
    "# import json\n",
    "# import requests\n",
    "\n",
    "# root_url = \"http://www.parliament.go.ke\"\n",
    "\n",
    "# def extract_mp_data(url):\n",
    "#   \"\"\"\n",
    "#   Extracts data for each Member of Parliament (MP) from a webpage.\n",
    "\n",
    "#   Args:\n",
    "#       url: The URL of the webpage containing the table.\n",
    "\n",
    "#   Returns:\n",
    "#       A list of dictionaries, where each dictionary represents an MP with their details.\n",
    "#   \"\"\"\n",
    "#   data = []\n",
    "#   response = requests.get(url)\n",
    "#   soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "#   # Find the table body (tbody element)\n",
    "#   tbody = soup.find('tbody')\n",
    "\n",
    "#   # Check if tbody exists\n",
    "#   if not tbody:\n",
    "#     return data\n",
    "\n",
    "#   # Extract data from each table row (class: mp)\n",
    "#   for row in tbody.find_all('tr', class_='mp'):\n",
    "#     mp_data = {}\n",
    "\n",
    "#     # Name cell (assuming second cell)\n",
    "#     name_cell = row.find_all('td')[0]\n",
    "#     mp_data['name'] = name_cell.text.strip()\n",
    "\n",
    "#     # County cell (assuming fourth cell)\n",
    "#     image_cell = row.find_all('td')[1]\n",
    "#     anchor_tag = image_cell.find('a')\n",
    "#     img_tag = anchor_tag.find('img')\n",
    "\n",
    "#     if img_tag:  # Check if anchor_tag exists before accessing attributes\n",
    "#       image_url = img_tag.get('src')\n",
    "#     else:\n",
    "#       image_url = None  # Handle cases where no anchor tag is found\n",
    "#     mp_data['photoUrl'] = root_url+image_url\n",
    "\n",
    "#     # County cell (assuming fourth cell)\n",
    "#     county_cell = row.find_all('td')[2]\n",
    "#     mp_data['county'] = county_cell.text.strip()\n",
    "\n",
    "#     # Constituency cell (assuming fifth cell)\n",
    "#     constituency_cell = row.find_all('td')[3]\n",
    "#     mp_data['constituency'] = constituency_cell.text.strip()\n",
    "\n",
    "#     # Party cell (assuming sixth cell)\n",
    "#     party_cell = row.find_all('td')[4]\n",
    "#     mp_data['party'] = party_cell.text.strip()\n",
    "\n",
    "#     # Status cell (assuming seventh cell)\n",
    "#     status_cell = row.find_all('td')[5]\n",
    "#     mp_data['status'] = status_cell.text.strip()\n",
    "\n",
    "#     # Add MP data to the list\n",
    "#     data.append(mp_data)\n",
    "\n",
    "#   return data\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#   # Replace with the actual URL of the members list\n",
    "#   url = \"http://www.parliament.go.ke/the-national-assembly/mps\"\n",
    "#   mp_data = extract_mp_data(url)\n",
    "\n",
    "#   # Convert data to JSON\n",
    "#   json_data = json.dumps(mp_data, indent=4)\n",
    "#   print(json_data)\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import requests\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "\n",
    "root_url = \"http://www.parliament.go.ke\"\n",
    "\n",
    "def extract_mp_data(url):\n",
    "  \"\"\"\n",
    "  Extracts data for each Member of Parliament (MP) from a webpage.\n",
    "\n",
    "  Args:\n",
    "      url: The URL of the webpage containing the table.\n",
    "\n",
    "  Returns:\n",
    "      A list of dictionaries, where each dictionary represents an MP with their details.\n",
    "  \"\"\"\n",
    "  data = []\n",
    "  profileLinks=[]\n",
    "  tablePages=[]\n",
    "  response = requests.get(url)\n",
    "  soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "  # Target the specific nav element\n",
    "  pager_nav = soup.find('nav', class_='pager', role='navigation')\n",
    "  if pager_nav:\n",
    "  # Find the last 'li' element within the 'ul' with class 'pager__items'\n",
    "    ul_tag = pager_nav.find('ul')\n",
    "    if ul_tag:\n",
    "      # Only call find_last if ul_tag is not None\n",
    "      # print(str(ul_tag))\n",
    "      li_elements = ul_tag.find_all('li')\n",
    "      # Check if any 'li' elements were found\n",
    "      if li_elements:\n",
    "          href = li_elements[-1].find('a').get('href')# Get the last 'li' element\n",
    "\n",
    "          # Parse the query parameters\n",
    "          print('last pagination: ',href)\n",
    "          # Split the href_value by '='\n",
    "          # Split the href_value by '='\n",
    "          parts = href.split('=')\n",
    "\n",
    "          # Extract the second part (which contains the digits)\n",
    "          pages = parts[-1]\n",
    "          for i in range(int(pages)+1):\n",
    "            tablePages.append(f\"{url}?page={i}\")\n",
    "          \n",
    "          print(\"pages: \",tablePages)\n",
    "\n",
    "      else:\n",
    "          print(\"No 'li' elements found in the ul\")\n",
    "\n",
    "  for link in tablePages:\n",
    "    # Your code here (will be executed for each element)\n",
    "    # print(\"LINK: \",link)\n",
    "    response_ = requests.get(link)\n",
    "    linkSoup = BeautifulSoup(response_.content, 'html.parser')\n",
    "\n",
    "    # Find the table body (tbody element)\n",
    "    tbody = linkSoup.find('tbody')\n",
    "\n",
    "    # Check if tbody exists\n",
    "    if not tbody:\n",
    "      continue\n",
    "\n",
    "\n",
    "    # Extract data from each table row (class: mp)\n",
    "    for row in tbody.find_all('tr', class_='mp'):\n",
    "      cells = row.find_all(\"td\")\n",
    "      mp_data = {}\n",
    "\n",
    "      # Name cell (assuming second cell)\n",
    "      name_cell = cells[0]\n",
    "      if not name_cell.text.strip():\n",
    "        continue\n",
    "      mp_data['name'] = name_cell.text.strip()\n",
    "      # print('name',name_cell.text.strip())\n",
    "\n",
    "      # County cell (assuming fourth cell)\n",
    "      image_cell = cells[1]\n",
    "      anchor_tag = image_cell.find('a')\n",
    "      img_tag = anchor_tag.find('img')\n",
    "\n",
    "      if img_tag:  # Check if anchor_tag exists before accessing attributes\n",
    "        image_url = img_tag.get('src')\n",
    "      else:\n",
    "        image_url = None  # Handle cases where no anchor tag is found\n",
    "      mp_data['photoUrl'] = root_url+image_url\n",
    "\n",
    "      # County cell (assuming 3rd cell)\n",
    "      county_cell = cells[2]\n",
    "      mp_data['county'] = county_cell.text.strip()\n",
    "\n",
    "      # Constituency cell (4th cell)\n",
    "      constituency_cell = cells[3]\n",
    "      mp_data['constituency'] = constituency_cell.text.strip()\n",
    "\n",
    "      # Party cell (5th cell)\n",
    "      party_cell = cells[4]\n",
    "      mp_data['party'] = party_cell.text.strip()\n",
    "\n",
    "      # Status cell (6th cell)\n",
    "      if len(cells) >= 6:  # Ensure there are at least 6 cells in the row\n",
    "        status_cell = cells[5]\n",
    "        mp_data['status'] = status_cell.text.strip()\n",
    "\n",
    "      # Add MP data to the list\n",
    "      data.append(mp_data)\n",
    "\n",
    "  return data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  # Replace with the actual URL of the members list\n",
    "  url = \"http://www.parliament.go.ke/the-national-assembly/mps\"\n",
    "  mp_data = extract_mp_data(url)\n",
    "  print(\"MPs count: \",len(mp_data))\n",
    "  # Convert data to JSON\n",
    "  json_data = json.dumps(mp_data, indent=4)\n",
    "  print(\"JSON: \",json_data)\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
